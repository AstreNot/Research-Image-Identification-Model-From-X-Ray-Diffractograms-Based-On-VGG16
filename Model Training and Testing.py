# -*- coding: utf-8 -*-
"""Testing Model 20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WIan73gcTDo4uw_7-Ui87EQWJEYbF9y2
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from glob import glob
from keras.models import Sequential, load_model
from keras.layers import Dense
from keras.applications.vgg16 import VGG16
from keras.applications.vgg19 import VGG19
from keras.utils import image_dataset_from_directory, load_img, array_to_img
from keras.optimizers import Adam
from tensorflow import expand_dims
import tensorflow as tf
from sklearn.metrics import confusion_matrix
import os

import warnings
warnings.filterwarnings("ignore")

train_path = '/content/drive/MyDrive/TrainingXRD/train'
test_path = '/content/drive/MyDrive/TrainingXRD/test'
val_path = '/content/drive/MyDrive/TrainingXRD/validation'

# The number of classes of dataset
numberOfClass = len(glob(train_path + "/*"))
print("Number Of Class: ", numberOfClass)

# Prepare the datasef for vgg16
#train_data = ImageDataGenerator().flow_from_directory(train_path, target_size = (224,224))
#test_data = ImageDataGenerator().flow_from_directory(test_path, target_size = (224,224))

train_data = image_dataset_from_directory(train_path, shuffle = True, image_size = (224,224), batch_size = 32, validation_split = False, labels = 'inferred', label_mode = 'categorical')
test_data = image_dataset_from_directory(test_path, shuffle = True, image_size = (224,224), batch_size = 32, validation_split = False, labels = 'inferred', label_mode = 'categorical')

vgg19 = VGG19()

# Layers of vgg16
vgg19.summary()

vgg19_layer_list = vgg19.layers
for i in vgg19_layer_list:
    print(i)

# add the layers of vgg16 in my created model.
vgg19Model = Sequential()
for i in range(len(vgg19_layer_list)-1):
    vgg19Model.add(vgg19_layer_list[i])

# the final version of the model
vgg19Model.summary()

# Close the layers of vgg16
for layers in vgg19Model.layers:
    layers.trainable = False

# Last layer
vgg19Model.add(Dense(numberOfClass, activation = "softmax"))

# After I added last layer in created model.
vgg19Model.summary()

# I create compile part.
opt = Adam(learning_rate = 0.003)

vgg19Model.compile(loss = "categorical_crossentropy",
             optimizer = "adam",
             metrics = ["accuracy"])

# Traning with model
batch_size = 32
model_filepath = '/content/drive/MyDrive/Model/Model_Vgg19_003.keras'
from keras import callbacks
earlystopping = callbacks.EarlyStopping(monitor="val_accuracy",
                                        mode="max",
                                        patience=15,
                                        restore_best_weights=True)

checkpoint = callbacks.ModelCheckpoint(filepath = model_filepath,
                           monitor='val_accuracy',
                           mode='max',
                           verbose = 0,
                           save_best_only =True)

data_history = callbacks.CSVLogger ('training_Vgg19_003.log', separator=',', append=False)


hist_vgg16 = vgg19Model.fit(train_data,
                                      steps_per_epoch = len(train_data) // batch_size,
                                      epochs = 100,
                                      validation_data = test_data,
                                      validation_steps = len(test_data) // batch_size,
                                      callbacks = [earlystopping, checkpoint, data_history])

# Loss and Validation Loss
plt.plot(hist_vgg16.history["loss"], label = "training loss")
plt.plot(hist_vgg16.history["val_loss"], label = "validation loss")
plt.legend()
plt.show()

# Accuracy and Validation Accuracy
plt.plot(hist_vgg16.history["accuracy"], label = "accuracy")
plt.plot(hist_vgg16.history["val_accuracy"], label = "validation accuracy")
plt.legend()
plt.show()

model2 = load_model('/content/drive/MyDrive/Model/Model 16.keras')
# summarize model.
model2.summary()
model2.evaluate(test_data)

image = 'vermiculite.png'
image = load_img(image, target_size= (224,224))
arr = array_to_img(image)
img_bat = expand_dims(arr, 0)
img_bat /= 255

predict = model2.predict (img_bat)

score = tf.nn.softmax(predict)

data_cat = train_data_cat.class_names

print('Results in image is {} with accuracy of {:0.2f}'.format(data_cat[np.argmax(score)], np.max(score)*100))

!pip install pandoc

!jupyter nbconvert --to html /content/Testing_Model_19.ipynb

